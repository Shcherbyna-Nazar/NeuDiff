{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b32ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `e:\\Proga\\AWID\\MyDiffMLP`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "cd(@__DIR__)\n",
    "cd(\"..\")\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "\n",
    "using MyDiffMLP\n",
    "using MyDiffMLP.MyAD\n",
    "using MyDiffMLP.MyNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = prepare_dataset(10000, 0.8)\n",
    "\n",
    "# X_train = data.X_train\n",
    "# y_train = data.y_train\n",
    "# X_test = data.X_test\n",
    "# y_test = data.y_test\n",
    "\n",
    "    \n",
    "# nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "facadf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12849"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JLD2\n",
    "X_train = load(\"data/imdb_dataset_prepared.jld2\", \"X_train\")\n",
    "y_train = load(\"data/imdb_dataset_prepared.jld2\", \"y_train\")\n",
    "X_test  = load(\"data/imdb_dataset_prepared.jld2\", \"X_test\")\n",
    "y_test  = load(\"data/imdb_dataset_prepared.jld2\", \"y_test\")\n",
    "embeddings = load(\"data/imdb_dataset_prepared.jld2\", \"embeddings\")\n",
    "vocab = load(\"data/imdb_dataset_prepared.jld2\", \"vocab\")\n",
    "\n",
    "embedding_dim = size(embeddings,1)\n",
    "vocab_size = length(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a0097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1 ===\n",
      "  → Training on 2500 batches of size 16...\n",
      "  → Batch 1\n",
      "    Forward pass...\n",
      "    Train loss: 2.6626 | acc: 0.5000\n",
      "    Backward pass...\n",
      "  4.512027 seconds (3.01 M allocations: 244.081 MiB, 1.11% gc time, 77.44% compilation time)\n",
      "  → Batch 2\n",
      "    Forward pass...\n",
      "    Train loss: 1.7524 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.872652 seconds (445.98 k allocations: 114.724 MiB, 1.74% gc time)\n",
      "  → Batch 3\n",
      "    Forward pass...\n",
      "    Train loss: 2.6889 | acc: 0.3750\n",
      "    Backward pass...\n",
      "  0.910604 seconds (445.98 k allocations: 114.724 MiB, 1.68% gc time)\n",
      "  → Batch 4\n",
      "    Forward pass...\n",
      "    Train loss: 1.0215 | acc: 0.7500\n",
      "    Backward pass...\n",
      "  0.894780 seconds (445.98 k allocations: 114.723 MiB, 1.79% gc time)\n",
      "  → Batch 5\n",
      "    Forward pass...\n",
      "    Train loss: 3.8225 | acc: 0.3125\n",
      "    Backward pass...\n",
      "  0.940852 seconds (445.98 k allocations: 114.722 MiB, 1.76% gc time)\n",
      "  → Batch 6\n",
      "    Forward pass...\n",
      "    Train loss: 3.5404 | acc: 0.3750\n",
      "    Backward pass...\n",
      "  0.871189 seconds (445.98 k allocations: 114.723 MiB, 1.68% gc time)\n",
      "  → Batch 7\n",
      "    Forward pass...\n",
      "    Train loss: 3.4131 | acc: 0.2500\n",
      "    Backward pass...\n",
      "  0.912963 seconds (445.98 k allocations: 114.722 MiB, 1.70% gc time)\n",
      "  → Batch 8\n",
      "    Forward pass...\n",
      "    Train loss: 2.0716 | acc: 0.5000\n",
      "    Backward pass...\n",
      "  0.961942 seconds (445.98 k allocations: 114.722 MiB, 1.85% gc time)\n",
      "  → Batch 9\n",
      "    Forward pass...\n",
      "    Train loss: 1.6691 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.878361 seconds (445.98 k allocations: 114.723 MiB, 1.74% gc time)\n",
      "  → Batch 10\n",
      "    Forward pass...\n",
      "    Train loss: 1.9415 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.919265 seconds (445.98 k allocations: 114.721 MiB, 1.65% gc time)\n",
      "  → Batch 11\n",
      "    Forward pass...\n",
      "    Train loss: 1.5169 | acc: 0.6250\n",
      "    Backward pass...\n",
      "  0.873621 seconds (445.98 k allocations: 114.722 MiB, 1.53% gc time)\n",
      "  → Batch 12\n",
      "    Forward pass...\n",
      "    Train loss: 2.8793 | acc: 0.5000\n",
      "    Backward pass...\n",
      "  1.049843 seconds (445.98 k allocations: 114.722 MiB, 17.69% gc time)\n",
      "  → Batch 13\n",
      "    Forward pass...\n",
      "    Train loss: 1.6186 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  1.065327 seconds (445.98 k allocations: 114.722 MiB, 2.61% gc time)\n",
      "  → Batch 14\n",
      "    Forward pass...\n",
      "    Train loss: 2.6283 | acc: 0.6250\n",
      "    Backward pass...\n",
      "  0.925967 seconds (445.98 k allocations: 114.725 MiB, 1.50% gc time)\n",
      "  → Batch 15\n",
      "    Forward pass...\n",
      "    Train loss: 2.0374 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.887266 seconds (445.98 k allocations: 114.724 MiB, 1.77% gc time)\n",
      "  → Batch 16\n",
      "    Forward pass...\n",
      "    Train loss: 3.7612 | acc: 0.2500\n",
      "    Backward pass...\n",
      "  0.882465 seconds (445.98 k allocations: 114.724 MiB, 1.54% gc time)\n",
      "  → Batch 17\n",
      "    Forward pass...\n",
      "    Train loss: 4.3638 | acc: 0.1875\n",
      "    Backward pass...\n",
      "  0.866799 seconds (445.98 k allocations: 114.725 MiB, 1.55% gc time)\n",
      "  → Batch 18\n",
      "    Forward pass...\n",
      "    Train loss: 2.6790 | acc: 0.5000\n",
      "    Backward pass...\n",
      "  0.903233 seconds (445.98 k allocations: 114.724 MiB, 1.55% gc time)\n",
      "  → Batch 19\n",
      "    Forward pass...\n",
      "    Train loss: 2.0412 | acc: 0.7500\n",
      "    Backward pass...\n",
      "  0.878334 seconds (445.98 k allocations: 114.724 MiB, 1.75% gc time)\n",
      "  → Batch 20\n",
      "    Forward pass...\n",
      "    Train loss: 2.8739 | acc: 0.3125\n",
      "    Backward pass...\n",
      "  0.862380 seconds (445.98 k allocations: 114.725 MiB, 1.42% gc time)\n",
      "  → Batch 21\n",
      "    Forward pass...\n",
      "    Train loss: 2.3480 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.902579 seconds (445.98 k allocations: 114.725 MiB, 1.69% gc time)\n",
      "  → Batch 22\n",
      "    Forward pass...\n",
      "    Train loss: 1.4196 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.850230 seconds (445.98 k allocations: 114.724 MiB, 1.70% gc time)\n",
      "  → Batch 23\n",
      "    Forward pass...\n",
      "    Train loss: 0.8660 | acc: 0.7500\n",
      "    Backward pass...\n",
      "  1.059891 seconds (445.98 k allocations: 114.724 MiB, 1.93% gc time)\n",
      "  → Batch 24\n",
      "    Forward pass...\n",
      "    Train loss: 2.4413 | acc: 0.5000\n",
      "    Backward pass...\n",
      "  0.887894 seconds (445.98 k allocations: 114.725 MiB, 1.63% gc time)\n",
      "  → Batch 25\n",
      "    Forward pass...\n",
      "    Train loss: 1.7584 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.901484 seconds (445.98 k allocations: 114.724 MiB, 1.59% gc time)\n",
      "  → Batch 26\n",
      "    Forward pass...\n",
      "    Train loss: 1.8189 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.984944 seconds (445.98 k allocations: 114.725 MiB, 1.34% gc time)\n",
      "  → Batch 27\n",
      "    Forward pass...\n",
      "    Train loss: 2.8937 | acc: 0.5000\n",
      "    Backward pass...\n",
      "  0.970611 seconds (445.98 k allocations: 114.724 MiB, 1.81% gc time)\n",
      "  → Batch 28\n",
      "    Forward pass...\n",
      "    Train loss: 2.1417 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  1.095584 seconds (445.98 k allocations: 114.724 MiB, 1.53% gc time)\n",
      "  → Batch 29\n",
      "    Forward pass...\n",
      "    Train loss: 2.3220 | acc: 0.5000\n",
      "    Backward pass...\n",
      "  1.150203 seconds (445.98 k allocations: 114.725 MiB, 1.57% gc time)\n",
      "  → Batch 30\n",
      "    Forward pass...\n",
      "    Train loss: 1.4343 | acc: 0.6875\n",
      "    Backward pass...\n",
      "  0.984843 seconds (445.98 k allocations: 114.721 MiB, 2.16% gc time)\n",
      "  → Batch 31\n",
      "    Forward pass...\n",
      "    Train loss: 2.4757 | acc: 0.3125\n",
      "    Backward pass...\n",
      "  0.890753 seconds (445.98 k allocations: 114.724 MiB, 1.52% gc time)\n",
      "  → Batch 32\n",
      "    Forward pass...\n",
      "    Train loss: 2.1646 | acc: 0.3750\n",
      "    Backward pass...\n",
      "  0.975608 seconds (445.98 k allocations: 114.725 MiB, 1.72% gc time)\n",
      "  → Batch 33\n",
      "    Forward pass...\n",
      "    Train loss: 1.5153 | acc: 0.6250\n",
      "    Backward pass...\n",
      "  0.884980 seconds (445.98 k allocations: 114.726 MiB, 1.66% gc time)\n",
      "  → Batch 34\n",
      "    Forward pass...\n",
      "    Train loss: 2.9814 | acc: 0.4375\n",
      "    Backward pass...\n",
      "  0.886100 seconds (445.98 k allocations: 114.725 MiB, 1.71% gc time)\n",
      "  → Batch 35\n",
      "    Forward pass...\n",
      "    Train loss: 2.1745 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.992068 seconds (445.98 k allocations: 114.725 MiB, 1.55% gc time)\n",
      "  → Batch 36\n",
      "    Forward pass...\n",
      "    Train loss: 3.2676 | acc: 0.5000\n",
      "    Backward pass...\n",
      "  0.960862 seconds (445.98 k allocations: 114.725 MiB, 1.67% gc time)\n",
      "  → Batch 37\n",
      "    Forward pass...\n",
      "    Train loss: 4.3164 | acc: 0.2500\n",
      "    Backward pass...\n",
      "  0.900596 seconds (445.98 k allocations: 114.724 MiB, 1.58% gc time)\n",
      "  → Batch 38\n",
      "    Forward pass...\n",
      "    Train loss: 2.1683 | acc: 0.3750\n",
      "    Backward pass...\n",
      "  0.938587 seconds (445.98 k allocations: 114.725 MiB, 1.67% gc time)\n",
      "  → Batch 39\n",
      "    Forward pass...\n",
      "    Train loss: 1.6949 | acc: 0.5000\n",
      "    Backward pass...\n",
      "  0.911532 seconds (445.98 k allocations: 114.726 MiB, 1.68% gc time)\n",
      "  → Batch 40\n",
      "    Forward pass...\n",
      "    Train loss: 2.6310 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.883925 seconds (445.98 k allocations: 114.726 MiB, 1.52% gc time)\n",
      "  → Batch 41\n",
      "    Forward pass...\n",
      "    Train loss: 1.6708 | acc: 0.5000\n",
      "    Backward pass...\n",
      "  1.024026 seconds (445.98 k allocations: 114.724 MiB, 1.78% gc time)\n",
      "  → Batch 42\n",
      "    Forward pass...\n",
      "    Train loss: 2.3890 | acc: 0.6250\n",
      "    Backward pass...\n",
      "  0.881584 seconds (445.98 k allocations: 114.724 MiB, 1.95% gc time)\n",
      "  → Batch 43\n",
      "    Forward pass...\n",
      "    Train loss: 1.0945 | acc: 0.7500\n",
      "    Backward pass...\n",
      "  0.875695 seconds (445.98 k allocations: 114.725 MiB, 1.57% gc time)\n",
      "  → Batch 44\n",
      "    Forward pass...\n",
      "    Train loss: 2.6857 | acc: 0.4375\n",
      "    Backward pass...\n",
      "  0.965778 seconds (445.98 k allocations: 114.725 MiB, 1.69% gc time)\n",
      "  → Batch 45\n",
      "    Forward pass...\n",
      "    Train loss: 2.5371 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  0.874420 seconds (445.98 k allocations: 114.725 MiB, 1.48% gc time)\n",
      "  → Batch 46\n",
      "    Forward pass...\n",
      "    Train loss: 2.2708 | acc: 0.3750\n",
      "    Backward pass...\n",
      "  0.911852 seconds (445.98 k allocations: 114.725 MiB, 1.82% gc time)\n",
      "  → Batch 47\n",
      "    Forward pass...\n",
      "    Train loss: 1.5995 | acc: 0.5625\n",
      "    Backward pass...\n",
      "  1.125511 seconds (445.98 k allocations: 114.724 MiB, 15.80% gc time)\n",
      "  → Batch 48\n",
      "    Forward pass...\n",
      "    Train loss: 2.4593 | acc: 0.5625\n",
      "    Backward pass...\n"
     ]
    }
   ],
   "source": [
    "using Printf, Statistics, Random\n",
    "using .MyAD\n",
    "using .MyNN\n",
    "\n",
    "# === Define model ===\n",
    "model = Chain(\n",
    "    Embedding(vocab_size, embedding_dim; pretrained_weights=embeddings),\n",
    "    Conv1D(embedding_dim, 8, 3, relu),\n",
    "    MaxPool1D(8, 8),\n",
    "    flatten_last_two_dims,\n",
    "    Dense(128, 1, sigmoid)\n",
    ")\n",
    "\n",
    "\n",
    "# === Define loss and accuracy ===\n",
    "function bce(ŷ, y)\n",
    "    ϵ = 1e-7\n",
    "    ŷ_clipped = clamp.(ŷ, ϵ, 1 .- ϵ)\n",
    "    return -mean(y .* log.(ŷ_clipped) .+ (1 .- y) .* log.(1 .- ŷ_clipped))\n",
    "end\n",
    "\n",
    "function bce_grad(ŷ, y)\n",
    "    ϵ = 1e-7\n",
    "    return (ŷ .- y) ./ clamp.(ŷ .* (1 .- ŷ), ϵ, 1.0) ./ size(y, 2)\n",
    "end\n",
    "\n",
    "accuracy(ŷ, y) = mean((ŷ .> 0.5) .== (y .> 0.5))\n",
    "\n",
    "# === Training settings ===\n",
    "params = parameters(model)\n",
    "state = AdamState(params)\n",
    "epochs = 5\n",
    "η = 0.001\n",
    "batch_size = 16\n",
    "\n",
    "function create_batches(X, Y; batchsize=64, shuffle=true)\n",
    "    idxs = collect(1:size(X, 2))\n",
    "    if shuffle\n",
    "        Random.shuffle!(idxs)\n",
    "    end\n",
    "    return [(X[:, idxs[i:min(i+batchsize-1, end)]],\n",
    "             Y[:, idxs[i:min(i+batchsize-1, end)]])\n",
    "             for i in 1:batchsize:length(idxs)]\n",
    "end\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in 1:epochs\n",
    "    println(\"=== Epoch $epoch ===\")\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    batches = create_batches(X_train, y_train, batchsize=batch_size)\n",
    "    println(\"  → Training on $(length(batches)) batches of size $batch_size...\")\n",
    "    \n",
    "    t = @elapsed begin\n",
    "        for (i, (x, y)) in enumerate(batches)\n",
    "            println(\"  → Batch $i\")\n",
    "\n",
    "            y_node = Variable(y, zeros(size(y)))\n",
    "            out = model(x)\n",
    "            graph = topological_sort(out)\n",
    "\n",
    "            println(\"    Forward pass...\")\n",
    "            forward!(graph)\n",
    "\n",
    "            ŷ = out.output\n",
    "            loss = bce(ŷ, y)\n",
    "            acc = accuracy(ŷ, y)\n",
    "\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "            num_batches += 1\n",
    "\n",
    "            println(@sprintf(\"    Train loss: %.4f | acc: %.4f\", loss, acc))\n",
    "\n",
    "            out.gradient = bce_grad(ŷ, y)\n",
    "            zero_gradients!(model)\n",
    "            println(\"    Backward pass...\")\n",
    "            @time backward!(graph, out.gradient)\n",
    "\n",
    "            update_adam!(state, params, η)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    train_loss = total_loss / num_batches\n",
    "    train_acc = total_acc / num_batches\n",
    "\n",
    "    println(\"  → Evaluation on test set...\")\n",
    "    out_eval = model(X_test)\n",
    "    forward!(topological_sort(out_eval))\n",
    "    test_pred = out_eval.output\n",
    "    test_loss = bce(test_pred, y_test)\n",
    "    test_acc = accuracy(test_pred, y_test)\n",
    "\n",
    "    println(@sprintf(\"✅ Epoch %d finished in %.2fs\", epoch, t))\n",
    "    println(@sprintf(\"🏋️  Train: loss = %.4f, acc = %.4f\", train_loss, train_acc))\n",
    "    println(@sprintf(\"🧪  Test : loss = %.4f, acc = %.4f\\n\", test_loss, test_acc))\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13d2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
