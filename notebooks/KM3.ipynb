{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b32ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `d:\\Proga\\AWID\\MyDiffMLP`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "cd(@__DIR__)\n",
    "cd(\"..\")\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "\n",
    "using NeuDiff\n",
    "using .NeuDiff.MyAD\n",
    "using .NeuDiff.MyNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64f8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = prepare_dataset(10000, 0.8)\n",
    "\n",
    "# X_train = data.X_train\n",
    "# y_train = data.y_train\n",
    "# X_test = data.X_test\n",
    "# y_test = data.y_test\n",
    "\n",
    "    \n",
    "# nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facadf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12849"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using JLD2\n",
    "X_train = load(\"data/imdb_dataset_prepared.jld2\", \"X_train\")\n",
    "y_train = load(\"data/imdb_dataset_prepared.jld2\", \"y_train\")\n",
    "X_test  = load(\"data/imdb_dataset_prepared.jld2\", \"X_test\")\n",
    "y_test  = load(\"data/imdb_dataset_prepared.jld2\", \"y_test\")\n",
    "embeddings = load(\"data/imdb_dataset_prepared.jld2\", \"embeddings\")\n",
    "vocab = load(\"data/imdb_dataset_prepared.jld2\", \"vocab\")\n",
    "\n",
    "embedding_dim = size(embeddings,1)\n",
    "vocab_size = length(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1a0097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1 ===\n",
      "  ‚Üí Training on 625 batches of size 64...\n",
      "    Batch 100/625: loss = 0.6912, acc = 0.5625\n",
      "    Batch 200/625: loss = 0.6163, acc = 0.7188\n",
      "    Batch 300/625: loss = 0.5702, acc = 0.7500\n",
      "    Batch 400/625: loss = 0.6218, acc = 0.6250\n",
      "    Batch 500/625: loss = 0.5425, acc = 0.7031\n",
      "    Batch 600/625: loss = 0.3847, acc = 0.8281\n",
      "    Batch 625/625: loss = 0.4253, acc = 0.7969\n",
      "  ‚Üí Evaluation on test set...\n",
      "‚úÖ Epoch 1 finished in 43.34s\n",
      "üèãÔ∏è  Train: loss = 0.5812, acc = 0.6783\n",
      "üß™  Test : loss = 0.4144, acc = 0.8176\n",
      "\n",
      "=== Epoch 2 ===\n",
      "  ‚Üí Training on 625 batches of size 64...\n",
      "    Batch 100/625: loss = 0.4191, acc = 0.8281\n",
      "    Batch 200/625: loss = 0.4974, acc = 0.7656\n",
      "    Batch 300/625: loss = 0.3311, acc = 0.8594\n",
      "    Batch 400/625: loss = 0.2289, acc = 0.9531\n",
      "    Batch 500/625: loss = 0.2632, acc = 0.8750\n",
      "    Batch 600/625: loss = 0.4147, acc = 0.7656\n",
      "    Batch 625/625: loss = 0.3525, acc = 0.8281\n",
      "  ‚Üí Evaluation on test set...\n",
      "‚úÖ Epoch 2 finished in 30.98s\n",
      "üèãÔ∏è  Train: loss = 0.3459, acc = 0.8516\n",
      "üß™  Test : loss = 0.3389, acc = 0.8570\n",
      "\n",
      "=== Epoch 3 ===\n",
      "  ‚Üí Training on 625 batches of size 64...\n",
      "    Batch 100/625: loss = 0.2070, acc = 0.9219\n",
      "    Batch 200/625: loss = 0.2138, acc = 0.9219\n",
      "    Batch 300/625: loss = 0.3207, acc = 0.8594\n",
      "    Batch 400/625: loss = 0.2342, acc = 0.9062\n",
      "    Batch 500/625: loss = 0.2195, acc = 0.9219\n",
      "    Batch 600/625: loss = 0.1484, acc = 0.9375\n",
      "    Batch 625/625: loss = 0.2286, acc = 0.8750\n",
      "  ‚Üí Evaluation on test set...\n",
      "‚úÖ Epoch 3 finished in 28.10s\n",
      "üèãÔ∏è  Train: loss = 0.2629, acc = 0.8942\n",
      "üß™  Test : loss = 0.3130, acc = 0.8685\n",
      "\n",
      "=== Epoch 4 ===\n",
      "  ‚Üí Training on 625 batches of size 64...\n",
      "    Batch 100/625: loss = 0.2685, acc = 0.9375\n",
      "    Batch 200/625: loss = 0.1657, acc = 0.9688\n",
      "    Batch 300/625: loss = 0.1684, acc = 0.9844\n",
      "    Batch 400/625: loss = 0.1400, acc = 0.9531\n",
      "    Batch 500/625: loss = 0.2383, acc = 0.8906\n",
      "    Batch 600/625: loss = 0.3831, acc = 0.8281\n",
      "    Batch 625/625: loss = 0.1503, acc = 0.9531\n",
      "  ‚Üí Evaluation on test set...\n",
      "‚úÖ Epoch 4 finished in 27.32s\n",
      "üèãÔ∏è  Train: loss = 0.2038, acc = 0.9231\n",
      "üß™  Test : loss = 0.3204, acc = 0.8718\n",
      "\n",
      "=== Epoch 5 ===\n",
      "  ‚Üí Training on 625 batches of size 64...\n",
      "    Batch 100/625: loss = 0.1058, acc = 0.9688\n",
      "    Batch 200/625: loss = 0.1684, acc = 0.9062\n",
      "    Batch 300/625: loss = 0.0764, acc = 1.0000\n",
      "    Batch 400/625: loss = 0.1299, acc = 0.9688\n",
      "    Batch 500/625: loss = 0.2184, acc = 0.9062\n",
      "    Batch 600/625: loss = 0.2210, acc = 0.9375\n",
      "    Batch 625/625: loss = 0.1163, acc = 0.9531\n",
      "  ‚Üí Evaluation on test set...\n",
      "‚úÖ Epoch 5 finished in 28.47s\n",
      "üèãÔ∏è  Train: loss = 0.1535, acc = 0.9465\n",
      "üß™  Test : loss = 0.3321, acc = 0.8690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using JLD2, Printf, Statistics, Random\n",
    "using TimerOutputs, LinearAlgebra\n",
    "\n",
    "# === Model ===\n",
    "model = Chain(\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    x -> PermuteDimsOp(x, (2, 1, 3)),  # (L, C, B) -> (C, L, B)\n",
    "    Conv1D(embedding_dim, 8, 3, relu),\n",
    "    MaxPool1D(8, 8),\n",
    "    flatten_last_two_dims,\n",
    "    Dense(128, 1, sigmoid)\n",
    ")\n",
    "model.layers[1].weight.output = embeddings\n",
    "\n",
    "\n",
    "# === Loss and accuracy ===\n",
    "function bce(yÃÇ, y)\n",
    "    œµ = 1e-7\n",
    "    yÃÇ_clipped = clamp.(yÃÇ, œµ, 1 .- œµ)\n",
    "    return -mean(y .* log.(yÃÇ_clipped) .+ (1 .- y) .* log.(1 .- yÃÇ_clipped))\n",
    "end\n",
    "\n",
    "function bce_grad(yÃÇ, y)\n",
    "    œµ = 1e-7\n",
    "    yÃÇ_clipped = clamp.(yÃÇ, œµ, 1 .- œµ)\n",
    "    return (yÃÇ_clipped .- y) ./ (yÃÇ_clipped .* (1 .- yÃÇ_clipped) * size(yÃÇ, 2))\n",
    "end\n",
    "\n",
    "accuracy(yÃÇ, y) = mean((yÃÇ .> 0.5) .== (y .> 0.5))\n",
    "\n",
    "# === Optimizer ===\n",
    "params = parameters(model)\n",
    "state = AdamState(params)\n",
    "Œ∑ = 0.001\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "# === Mini-batch generator ===\n",
    "function create_batches(X, Y; batchsize=64, shuffle=true)\n",
    "    idxs = collect(1:size(X, 2))\n",
    "    if shuffle\n",
    "        Random.shuffle!(idxs)\n",
    "    end\n",
    "    return [(X[:, idxs[i:min(i+batchsize-1, end)]],\n",
    "             Y[:, idxs[i:min(i+batchsize-1, end)]])\n",
    "             for i in 1:batchsize:length(idxs)]\n",
    "end\n",
    "\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in 1:epochs\n",
    "    println(\"=== Epoch $epoch ===\")\n",
    "    total_loss, total_acc, num_batches = 0.0, 0.0, 0\n",
    "    batches = create_batches(X_train, y_train, batchsize=batch_size)\n",
    "    println(\"  ‚Üí Training on $(length(batches)) batches of size $batch_size...\")\n",
    "\n",
    "    t = @elapsed begin\n",
    "        for (i, (x, y)) in enumerate(batches)\n",
    "            out = model(x)\n",
    "            graph = topological_sort(out)\n",
    "            forward!(graph)\n",
    "\n",
    "            yÃÇ = out.output\n",
    "            loss = bce(yÃÇ, y)\n",
    "            acc = accuracy(yÃÇ, y)\n",
    "\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "            num_batches += 1\n",
    "\n",
    "            zero_gradients!(model)\n",
    "            out.gradient = bce_grad(yÃÇ, y)\n",
    "            backward!(graph, out.gradient)\n",
    "            update_adam!(state, params, Œ∑)\n",
    "\n",
    "            if i % 100 == 0 || i == length(batches)\n",
    "                println(@sprintf(\"    Batch %d/%d: loss = %.4f, acc = %.4f\", i, length(batches), loss, acc))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    train_loss = total_loss / num_batches\n",
    "    train_acc = total_acc / num_batches\n",
    "\n",
    "    # === Evaluation ===\n",
    "    println(\"  ‚Üí Evaluation on test set...\")\n",
    "    out_eval = model(X_test)\n",
    "    forward!(topological_sort(out_eval))\n",
    "    test_pred = out_eval.output\n",
    "    test_loss = bce(test_pred, y_test)\n",
    "    test_acc = accuracy(test_pred, y_test)\n",
    "\n",
    "    println(@sprintf(\"‚úÖ Epoch %d finished in %.2fs\", epoch, t))\n",
    "    println(@sprintf(\"üèãÔ∏è  Train: loss = %.4f, acc = %.4f\", train_loss, train_acc))\n",
    "    println(@sprintf(\"üß™  Test : loss = %.4f, acc = %.4f\\n\", test_loss, test_acc))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f13d2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
