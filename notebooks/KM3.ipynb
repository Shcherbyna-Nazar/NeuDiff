{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b32ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `d:\\Proga\\AWID\\MyDiffMLP`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "cd(@__DIR__)\n",
    "cd(\"..\")\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "\n",
    "using NeuDiff\n",
    "using .NeuDiff.MyAD\n",
    "using .NeuDiff.MyNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = prepare_dataset(10000, 0.8)\n",
    "\n",
    "# X_train = data.X_train\n",
    "# y_train = data.y_train\n",
    "# X_test = data.X_test\n",
    "# y_test = data.y_test\n",
    "\n",
    "    \n",
    "# nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facadf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12849"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using JLD2\n",
    "X_train = load(\"data/imdb_dataset_prepared.jld2\", \"X_train\")\n",
    "y_train = load(\"data/imdb_dataset_prepared.jld2\", \"y_train\")\n",
    "X_test  = load(\"data/imdb_dataset_prepared.jld2\", \"X_test\")\n",
    "y_test  = load(\"data/imdb_dataset_prepared.jld2\", \"y_test\")\n",
    "embeddings = load(\"data/imdb_dataset_prepared.jld2\", \"embeddings\")\n",
    "vocab = load(\"data/imdb_dataset_prepared.jld2\", \"vocab\")\n",
    "\n",
    "embedding_dim = size(embeddings,1)\n",
    "vocab_size = length(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1a0097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1 ===\n",
      "  ‚Üí Training on 625 batches of size 64...\n",
      "    Batch 100/625: loss = 0.6693, acc = 0.5781\n",
      "    Batch 200/625: loss = 0.6016, acc = 0.6406\n",
      "    Batch 300/625: loss = 0.5918, acc = 0.7031\n",
      "    Batch 400/625: loss = 0.4715, acc = 0.7656\n",
      "    Batch 500/625: loss = 0.4302, acc = 0.7656\n",
      "    Batch 600/625: loss = 0.4234, acc = 0.7812\n",
      "    Batch 625/625: loss = 0.4388, acc = 0.7969\n",
      "  ‚Üí Evaluation on test set...\n",
      "‚úÖ Epoch 1 finished in 42.66s\n",
      "üèãÔ∏è  Train: loss = 0.5430, acc = 0.7142\n",
      "üß™  Test : loss = 0.3842, acc = 0.8312\n",
      "\n",
      "=== Epoch 2 ===\n",
      "  ‚Üí Training on 625 batches of size 64...\n",
      "    Batch 100/625: loss = 0.5510, acc = 0.7969\n",
      "    Batch 200/625: loss = 0.3785, acc = 0.8125\n",
      "    Batch 300/625: loss = 0.3820, acc = 0.8438\n",
      "    Batch 400/625: loss = 0.3482, acc = 0.8594\n",
      "    Batch 500/625: loss = 0.3089, acc = 0.8750\n",
      "    Batch 600/625: loss = 0.3648, acc = 0.7812\n",
      "    Batch 625/625: loss = 0.3542, acc = 0.7969\n",
      "  ‚Üí Evaluation on test set...\n",
      "‚úÖ Epoch 2 finished in 32.52s\n",
      "üèãÔ∏è  Train: loss = 0.3289, acc = 0.8609\n",
      "üß™  Test : loss = 0.3205, acc = 0.8638\n",
      "\n",
      "=== Epoch 3 ===\n",
      "  ‚Üí Training on 625 batches of size 64...\n",
      "    Batch 100/625: loss = 0.4420, acc = 0.8438\n",
      "    Batch 200/625: loss = 0.1762, acc = 0.9062\n",
      "    Batch 300/625: loss = 0.2203, acc = 0.9531\n",
      "    Batch 400/625: loss = 0.1769, acc = 0.9375\n",
      "    Batch 500/625: loss = 0.1129, acc = 0.9688\n",
      "    Batch 600/625: loss = 0.2557, acc = 0.8906\n",
      "    Batch 625/625: loss = 0.3009, acc = 0.8594\n",
      "  ‚Üí Evaluation on test set...\n",
      "‚úÖ Epoch 3 finished in 27.40s\n",
      "üèãÔ∏è  Train: loss = 0.2512, acc = 0.8995\n",
      "üß™  Test : loss = 0.3087, acc = 0.8713\n",
      "\n",
      "=== Epoch 4 ===\n",
      "  ‚Üí Training on 625 batches of size 64...\n",
      "    Batch 100/625: loss = 0.1848, acc = 0.9531\n",
      "    Batch 200/625: loss = 0.1665, acc = 0.9375\n",
      "    Batch 300/625: loss = 0.1430, acc = 0.9688\n",
      "    Batch 400/625: loss = 0.2801, acc = 0.9062\n",
      "    Batch 500/625: loss = 0.2095, acc = 0.9219\n",
      "    Batch 600/625: loss = 0.1307, acc = 0.9688\n",
      "    Batch 625/625: loss = 0.2147, acc = 0.9375\n",
      "  ‚Üí Evaluation on test set...\n",
      "‚úÖ Epoch 4 finished in 25.80s\n",
      "üèãÔ∏è  Train: loss = 0.1941, acc = 0.9285\n",
      "üß™  Test : loss = 0.3152, acc = 0.8732\n",
      "\n",
      "=== Epoch 5 ===\n",
      "  ‚Üí Training on 625 batches of size 64...\n",
      "    Batch 100/625: loss = 0.1242, acc = 0.9688\n",
      "    Batch 200/625: loss = 0.2355, acc = 0.9219\n",
      "    Batch 300/625: loss = 0.1458, acc = 0.9219\n",
      "    Batch 400/625: loss = 0.1486, acc = 0.9375\n",
      "    Batch 500/625: loss = 0.1890, acc = 0.8906\n",
      "    Batch 600/625: loss = 0.2598, acc = 0.8906\n",
      "    Batch 625/625: loss = 0.2198, acc = 0.9688\n",
      "  ‚Üí Evaluation on test set...\n",
      "‚úÖ Epoch 5 finished in 26.72s\n",
      "üèãÔ∏è  Train: loss = 0.1447, acc = 0.9514\n",
      "üß™  Test : loss = 0.3350, acc = 0.8711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using JLD2, Printf, Statistics, Random\n",
    "using TimerOutputs, LinearAlgebra\n",
    "\n",
    "# === Model ===\n",
    "model = Chain(\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    x -> PermuteDimsOp(x, (2, 1, 3)),  # (L, C, B) -> (C, L, B)\n",
    "    Conv1D(embedding_dim, 8, 3, relu),\n",
    "    MaxPool1D(8, 8),\n",
    "    flatten_last_two_dims,\n",
    "    Dense(128, 1, sigmoid)\n",
    ")\n",
    "model.layers[1].weight.output = embeddings\n",
    "\n",
    "\n",
    "# === Loss and accuracy ===\n",
    "function bce(yÃÇ, y)\n",
    "    œµ = 1e-7\n",
    "    yÃÇ_clipped = clamp.(yÃÇ, œµ, 1 .- œµ)\n",
    "    return -mean(y .* log.(yÃÇ_clipped) .+ (1 .- y) .* log.(1 .- yÃÇ_clipped))\n",
    "end\n",
    "\n",
    "function bce_grad(yÃÇ, y)\n",
    "    œµ = 1e-7\n",
    "    yÃÇ_clipped = clamp.(yÃÇ, œµ, 1 .- œµ)\n",
    "    return (yÃÇ_clipped .- y) ./ (yÃÇ_clipped .* (1 .- yÃÇ_clipped) * size(yÃÇ, 2))\n",
    "end\n",
    "\n",
    "accuracy(yÃÇ, y) = mean((yÃÇ .> 0.5) .== (y .> 0.5))\n",
    "\n",
    "# === Optimizer ===\n",
    "params = parameters(model)\n",
    "state = AdamState(params)\n",
    "Œ∑ = 0.001\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "# === Mini-batch generator ===\n",
    "function create_batches(X, Y; batchsize=64, shuffle=true)\n",
    "    idxs = collect(1:size(X, 2))\n",
    "    if shuffle\n",
    "        Random.shuffle!(idxs)\n",
    "    end\n",
    "    return [(X[:, idxs[i:min(i+batchsize-1, end)]],\n",
    "             Y[:, idxs[i:min(i+batchsize-1, end)]])\n",
    "             for i in 1:batchsize:length(idxs)]\n",
    "end\n",
    "\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in 1:epochs\n",
    "    println(\"=== Epoch $epoch ===\")\n",
    "    total_loss, total_acc, num_batches = 0.0, 0.0, 0\n",
    "    batches = create_batches(X_train, y_train, batchsize=batch_size)\n",
    "    println(\"  ‚Üí Training on $(length(batches)) batches of size $batch_size...\")\n",
    "\n",
    "    t = @elapsed begin\n",
    "        for (i, (x, y)) in enumerate(batches)\n",
    "            out = model(x)\n",
    "            graph = topological_sort(out)\n",
    "            forward!(graph)\n",
    "\n",
    "            yÃÇ = out.output\n",
    "            loss = bce(yÃÇ, y)\n",
    "            acc = accuracy(yÃÇ, y)\n",
    "\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "            num_batches += 1\n",
    "\n",
    "            zero_gradients!(model)\n",
    "            out.gradient = bce_grad(yÃÇ, y)\n",
    "            backward!(graph, out.gradient)\n",
    "            update_adam!(state, params, Œ∑)\n",
    "\n",
    "            if i % 100 == 0 || i == length(batches)\n",
    "                println(@sprintf(\"    Batch %d/%d: loss = %.4f, acc = %.4f\", i, length(batches), loss, acc))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    train_loss = total_loss / num_batches\n",
    "    train_acc = total_acc / num_batches\n",
    "\n",
    "    # === Evaluation ===\n",
    "    println(\"  ‚Üí Evaluation on test set...\")\n",
    "    out_eval = model(X_test)\n",
    "    forward!(topological_sort(out_eval))\n",
    "    test_pred = out_eval.output\n",
    "    test_loss = bce(test_pred, y_test)\n",
    "    test_acc = accuracy(test_pred, y_test)\n",
    "\n",
    "    println(@sprintf(\"‚úÖ Epoch %d finished in %.2fs\", epoch, t))\n",
    "    println(@sprintf(\"üèãÔ∏è  Train: loss = %.4f, acc = %.4f\", train_loss, train_acc))\n",
    "    println(@sprintf(\"üß™  Test : loss = %.4f, acc = %.4f\\n\", test_loss, test_acc))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13d2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
