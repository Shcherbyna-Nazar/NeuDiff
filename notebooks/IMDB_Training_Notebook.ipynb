{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b32ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `d:\\Proga\\AWID\\MyDiffMLP`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "cd(@__DIR__)\n",
    "cd(\"..\")\n",
    "Pkg.activate(\".\")\n",
    "using MyDiffMLP\n",
    "using MyDiffMLP.MyAD\n",
    "using MyDiffMLP.MyNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64f8cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading raw dataset...\n",
      "└ @ MyDiffMLP.PrepareData d:\\Proga\\AWID\\MyDiffMLP\\src\\data_prep.jl:10\n",
      "┌ Info: Data preparation...\n",
      "└ @ MyDiffMLP.PrepareData d:\\Proga\\AWID\\MyDiffMLP\\src\\data_prep.jl:17\n"
     ]
    }
   ],
   "source": [
    "data = prepare_dataset(10000, 0.8)\n",
    "\n",
    "X_train = data.X_train\n",
    "y_train = data.y_train\n",
    "X_test = data.X_test\n",
    "y_test = data.y_test\n",
    "\n",
    "    \n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "facadf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset is already prepared, you can load it directly\n",
    "using JLD2\n",
    "X_train = load(\"data/km2/imdb_dataset_prepared.jld2\", \"X_train\")\n",
    "y_train = load(\"data/km2/imdb_dataset_prepared.jld2\", \"y_train\")\n",
    "X_test = load(\"data/km2/imdb_dataset_prepared.jld2\", \"X_test\")\n",
    "y_test = load(\"data/km2/imdb_dataset_prepared.jld2\", \"y_test\")\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a0097",
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching (::Dense{Float32, typeof(relu)})(::Matrix{Float32})\nThe object of type `Dense{Float32, typeof(relu)}` exists, but no method is defined for this combination of argument types when trying to treat it as a callable object.\n\nClosest candidates are:\n  (::Dense)(!Matched::GraphNode)\n   @ MyDiffMLP d:\\Proga\\AWID\\MyDiffMLP\\src\\MyNN.jl:23\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching (::Dense{Float32, typeof(relu)})(::Matrix{Float32})\n",
      "The object of type `Dense{Float32, typeof(relu)}` exists, but no method is defined for this combination of argument types when trying to treat it as a callable object.\n",
      "\n",
      "Closest candidates are:\n",
      "  (::Dense)(!Matched::GraphNode)\n",
      "   @ MyDiffMLP d:\\Proga\\AWID\\MyDiffMLP\\src\\MyNN.jl:23\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      " [1] (::Chain{Tuple{Dense{Float32, typeof(relu)}, Dense{Float32, typeof(sigmoid)}}})(x::Matrix{Float32})\n",
      "   @ MyDiffMLP.MyNN d:\\Proga\\AWID\\MyDiffMLP\\src\\MyNN.jl:104\n",
      " [2] macro expansion\n",
      "   @ d:\\Proga\\AWID\\MyDiffMLP\\notebooks\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:47 [inlined]\n",
      " [3] macro expansion\n",
      "   @ .\\timing.jl:421 [inlined]\n",
      " [4] top-level scope\n",
      "   @ d:\\Proga\\AWID\\MyDiffMLP\\notebooks\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:44"
     ]
    }
   ],
   "source": [
    "using Printf, Statistics, Random\n",
    "\n",
    "function create_batches(X, Y; batchsize=64, shuffle=true)\n",
    "    idxs = collect(1:size(X, 2))\n",
    "    if shuffle\n",
    "        Random.shuffle!(idxs)\n",
    "    end\n",
    "    return [(X[:, idxs[i:min(i+batchsize-1, end)]],\n",
    "             Y[:, idxs[i:min(i+batchsize-1, end)]])\n",
    "             for i in 1:batchsize:length(idxs)]\n",
    "end\n",
    "model = Chain(\n",
    "    Dense(size(X_train, 1), 32, relu),\n",
    "    Dense(32, 1, sigmoid)\n",
    ")\n",
    "\n",
    "function bce(ŷ, y)\n",
    "    ϵ = 1e-7\n",
    "    ŷ_clipped = clamp.(ŷ, ϵ, 1 .- ϵ)\n",
    "    return -mean(y .* log.(ŷ_clipped) .+ (1 .- y) .* log.(1 .- ŷ_clipped))\n",
    "end\n",
    "\n",
    "function bce_grad(ŷ, y)\n",
    "    ϵ = 1e-7\n",
    "    return (ŷ .- y) ./ (clamp.(ŷ .* (1 .- ŷ), ϵ, 1.0)) ./ size(y, 2)\n",
    "end\n",
    "\n",
    "accuracy(ŷ, y) = mean((ŷ .> 0.5) .== (y .> 0.5))\n",
    "\n",
    "epochs = 5\n",
    "batchsize = 64\n",
    "η = 0.001\n",
    "\n",
    "params = parameters(model)\n",
    "state = AdamState(params)\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    batches = create_batches(X_train, y_train; batchsize=batchsize)\n",
    "\n",
    "    t = @elapsed begin\n",
    "        for (x, y) in batches\n",
    "            x_var = Variable(x)\n",
    "            out = model(x_var)\n",
    "            graph = topological_sort(out)\n",
    "            forward!(graph)\n",
    "\n",
    "            ŷ = out.output\n",
    "            l = bce(ŷ, y)\n",
    "            total_loss += l\n",
    "            total_acc += accuracy(ŷ, y)\n",
    "            num_batches += 1\n",
    "\n",
    "            out.gradient = bce_grad(ŷ, y)\n",
    "\n",
    "            zero_gradients!(model)\n",
    "            backward!(graph, out.gradient)\n",
    "            update_adam!(state, params, η)\n",
    "\n",
    "        end\n",
    "    end\n",
    "\n",
    "    train_loss = total_loss / num_batches\n",
    "    train_acc = total_acc / num_batches\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    x_test_var = Variable(X_test, zeros(Float32, size(X_test)))  # Wrap test data as a Variable\n",
    "    out_eval = model(x_test_var)\n",
    "    forward!(topological_sort(out_eval))\n",
    "    test_pred = out_eval.output\n",
    "    test_loss = bce(test_pred, y_test)\n",
    "    test_acc = accuracy(test_pred, y_test)\n",
    "\n",
    "    println(@sprintf(\"Epoch: %d (%.2fs) \\tTrain: (l: %.2f, a: %.2f) \\tTest: (l: %.2f, a: %.2f)\",\n",
    "        epoch, t, train_loss, train_acc, test_loss, test_acc))\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
