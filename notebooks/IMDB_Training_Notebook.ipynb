{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b32ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `d:\\Proga\\AWID\\MyDiffMLP`\n",
      "\u001b[92m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "   3003.5 ms\u001b[33m  ✓ \u001b[39mMyDiffMLP\n",
      "  1 dependency successfully precompiled in 4 seconds. 214 already precompiled.\n",
      "  \u001b[33m1\u001b[39m dependency precompiled but a different version is currently loaded. Restart julia to access the new version. Otherwise, loading dependents of this package may trigger further precompilation to work with the unexpected version.\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "cd(@__DIR__)\n",
    "cd(\"..\")\n",
    "Pkg.activate(\".\")\n",
    "using MyDiffMLP\n",
    "using MyDiffMLP.MyAD\n",
    "using MyDiffMLP.MyNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d64f8cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading raw dataset...\n",
      "└ @ MyDiffMLP.PrepareData d:\\Proga\\AWID\\MyDiffMLP\\src\\data_prep.jl:10\n",
      "┌ Info: Data preparation...\n",
      "└ @ MyDiffMLP.PrepareData d:\\Proga\\AWID\\MyDiffMLP\\src\\data_prep.jl:17\n"
     ]
    }
   ],
   "source": [
    "data = prepare_dataset(10000, 0.8)\n",
    "\n",
    "X_train = data.X_train\n",
    "y_train = data.y_train\n",
    "X_test = data.X_test\n",
    "y_test = data.y_test\n",
    "\n",
    "    \n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facadf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If dataset is already prepared, you can load it directly\n",
    "# using JLD2\n",
    "# X_train = load(\"data/km2/imdb_dataset_prepared.jld2\", \"X_train\")\n",
    "# y_train = load(\"data/km2/imdb_dataset_prepared.jld2\", \"y_train\")\n",
    "# X_test = load(\"data/km2/imdb_dataset_prepared.jld2\", \"X_test\")\n",
    "# y_test = load(\"data/km2/imdb_dataset_prepared.jld2\", \"y_test\")\n",
    "# nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d1a0097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (2.45s) \tTrain: (l: 0.65, a: 0.81) \tTest: (l: 0.59, a: 0.85)\n",
      "Epoch: 2 (2.16s) \tTrain: (l: 0.47, a: 0.92) \tTest: (l: 0.44, a: 0.86)\n",
      "Epoch: 3 (2.27s) \tTrain: (l: 0.30, a: 0.94) \tTest: (l: 0.37, a: 0.87)\n",
      "Epoch: 4 (2.32s) \tTrain: (l: 0.21, a: 0.96) \tTest: (l: 0.33, a: 0.87)\n",
      "Epoch: 5 (2.03s) \tTrain: (l: 0.15, a: 0.97) \tTest: (l: 0.32, a: 0.87)\n"
     ]
    }
   ],
   "source": [
    "using Printf, Statistics, Random\n",
    "\n",
    "function create_batches(X, Y; batchsize=64, shuffle=true)\n",
    "    idxs = collect(1:size(X, 2))\n",
    "    if shuffle\n",
    "        Random.shuffle!(idxs)\n",
    "    end\n",
    "    return [(X[:, idxs[i:min(i+batchsize-1, end)]],\n",
    "             Y[:, idxs[i:min(i+batchsize-1, end)]])\n",
    "             for i in 1:batchsize:length(idxs)]\n",
    "end\n",
    "model = Chain(\n",
    "    Dense(size(X_train, 1), 32, relu),\n",
    "    Dense(32, 1, sigmoid)\n",
    ")\n",
    "\n",
    "function bce(ŷ, y)\n",
    "    ϵ = 1e-7\n",
    "    ŷ_clipped = clamp.(ŷ, ϵ, 1 .- ϵ)\n",
    "    return -mean(y .* log.(ŷ_clipped) .+ (1 .- y) .* log.(1 .- ŷ_clipped))\n",
    "end\n",
    "\n",
    "function bce_grad(ŷ, y)\n",
    "    ϵ = 1e-7\n",
    "    return (ŷ .- y) ./ (clamp.(ŷ .* (1 .- ŷ), ϵ, 1.0)) ./ size(y, 2)\n",
    "end\n",
    "\n",
    "accuracy(ŷ, y) = mean((ŷ .> 0.5) .== (y .> 0.5))\n",
    "\n",
    "epochs = 5\n",
    "batchsize = 64\n",
    "η = 0.001\n",
    "\n",
    "params = parameters(model)\n",
    "state = AdamState(params)\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    batches = create_batches(X_train, y_train; batchsize=batchsize)\n",
    "\n",
    "    t = @elapsed begin\n",
    "        for (x, y) in batches\n",
    "    \n",
    "            x_var = Variable(x, zeros(Float32, size(x)))  # Wrap x as a Variable\n",
    "            out = model(x_var)\n",
    "            graph = topological_sort(out)\n",
    "            forward!(graph)\n",
    "\n",
    "            ŷ = out.output\n",
    "            l = bce(ŷ, y)\n",
    "            total_loss += l\n",
    "            total_acc += accuracy(ŷ, y)\n",
    "            num_batches += 1\n",
    "\n",
    "            out.gradient = bce_grad(ŷ, y)\n",
    "\n",
    "            zero_gradients!(model)\n",
    "            backward!(graph, out.gradient)\n",
    "            update_adam!(state, params, η)\n",
    "\n",
    "        end\n",
    "    end\n",
    "\n",
    "    train_loss = total_loss / num_batches\n",
    "    train_acc = total_acc / num_batches\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    x_test_var = Variable(X_test, zeros(Float32, size(X_test)))  # Wrap test data as a Variable\n",
    "    out_eval = model(x_test_var)\n",
    "    forward!(topological_sort(out_eval))\n",
    "    test_pred = out_eval.output\n",
    "    test_loss = bce(test_pred, y_test)\n",
    "    test_acc = accuracy(test_pred, y_test)\n",
    "\n",
    "    println(@sprintf(\"Epoch: %d (%.2fs) \\tTrain: (l: %.2f, a: %.2f) \\tTest: (l: %.2f, a: %.2f)\",\n",
    "        epoch, t, train_loss, train_acc, test_loss, test_acc))\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
