
# ğŸ§  NeuDiff â€” Modular Neural Network & Automatic Differentiation in Julia

**NeuDiff** is a lightweight, educational deep learning framework written in Julia, built from scratch for transparency, extensibility, and clarity. It features:

- a custom **reverse-mode automatic differentiation engine** (`MyAD`)
- a modular **neural network system** (`MyNN`)
- full support for layers like `Dense`, `Embedding`, `Conv1D`, `MaxPool1D`, `Chain`
- optimizers including **Adam**
- utilities for text and tabular data (e.g. IMDB sentiment classification pipeline)
- reproducible demos on both **NLP** and **toy datasets** (like the two spirals)

---

## ğŸ“¦ Features

- âœ… Explicit computation graph reverse-mode autodiff engine (`MyAD`)
- âœ… Modular neural network layers (`Dense`, `Embedding`, `Conv1D`, `MaxPool1D`, `Chain`, `Dropout`)
- âœ… Full training pipeline with cross-entropy loss and Adam optimizer
- âœ… Text preprocessing, tokenization, and embedding utilities
- âœ… Training and evaluation scripts for both text (IMDB) and tabular data
- âœ… Extensive unit tests vs. Flux/Zygote

---

## ğŸ“ Project Structure

```
NeuDiff/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ km2/
â”‚   â”œâ”€â”€ glove_6B_50d.jld2
â”‚   â”œâ”€â”€ imdb_dataset.jld2
â”‚   â””â”€â”€ imdb_dataset_prepared.jld2
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ IMDB_Training_Notebook.ipynb
â”‚   â””â”€â”€ KM3.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ MyAD/
â”‚   â”‚   â”œâ”€â”€ activations.jl      # activation functions (relu, sigmoid, etc.)
â”‚   â”‚   â”œâ”€â”€ exports.jl          # exported symbols for MyAD
â”‚   â”‚   â”œâ”€â”€ gradients.jl        # backward pass logic
â”‚   â”‚   â”œâ”€â”€ graph.jl            # computation graph utilities (topological sort, etc.)
â”‚   â”‚   â”œâ”€â”€ MyAD.jl             # main module for MyAD
â”‚   â”‚   â”œâ”€â”€ nodes.jl            # node type definitions (Variable, Constant, etc.)
â”‚   â”‚   â””â”€â”€ ops.jl              # operator nodes (matmul, scalar ops, etc.)
â”‚   â”œâ”€â”€ MyNN/
â”‚   â”‚   â”œâ”€â”€ exports.jl          # exported symbols for MyNN
â”‚   â”‚   â”œâ”€â”€ layers.jl           # neural network layer definitions
â”‚   â”‚   â”œâ”€â”€ MyNN.jl             # main module for MyNN
â”‚   â”‚   â”œâ”€â”€ optim.jl            # optimizers (Adam, SGD, etc.)
â”‚   â”‚   â””â”€â”€ utils.jl            # utility functions (parameters, zero_gradients!, etc.)
â”‚   â”œâ”€â”€ data_prep.jl            # IMDB dataset preprocessing pipeline
â”‚   â””â”€â”€ NeuDiff.jl              # main project module (exports MyAD and MyNN)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ performance/
â”‚   â”‚   â”œâ”€â”€ complex_model.jl
â”‚   â”‚   â”œâ”€â”€ conv1d.jl
â”‚   â”‚   â”œâ”€â”€ dense.jl
â”‚   â”‚   â”œâ”€â”€ embedding.jl
â”‚   â”‚   â””â”€â”€ maxpool1d.jl
â”‚   â”œâ”€â”€ correctness_tests.jl    # extensive unit tests vs Flux/Zygote
â”‚   â””â”€â”€ km3_test.jl
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Manifest.toml
â”œâ”€â”€ MLP_TFIDF_myAD_myNN.jl      # (example training script)
â”œâ”€â”€ Project.toml
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Clone and activate the environment

```julia
using Pkg
Pkg.activate(".")
Pkg.instantiate()
```

### 2. Prepare the IMDB dataset (if needed)

```julia
include("src/data_prep.jl")
data = prepare_dataset(10000, 0.8)
```

### 3. Train a model

You can use:
- the sample [notebook](notebooks/IMDB_Training_Notebook.ipynb)
- or your own training script, e.g.
  ```julia
  include("src/NeuDiff.jl")
  using .NeuDiff
  # ...build and train your model...
  ```

---

## ğŸ§ª Testing

Run correctness tests for your AD engine and NN layers:
```julia
include("tests/correctness_tests.jl")
```

---

## ğŸ“Š Sample Output (IMDB binary classification)

```
Epoch: 1     Train Loss: 0.67     Train Acc: 0.60     Test Acc: 0.58
Epoch: 5     Train Loss: 0.21     Train Acc: 0.94     Test Acc: 0.87
...
```

---

## ğŸ“š Dependencies

- Julia `1.9+`
- `JLD2.jl`, `TextAnalysis.jl`, `Flux.jl`, `FileIO.jl`, `IJulia`, `Statistics`

Install with:
```julia
Pkg.instantiate()
```

---

## ğŸ§  Why NeuDiff?

- **Educational:** Designed to reveal how neural nets and autodiff work under the hood.
- **Modular:** Add new ops/layers with ease.
- **Tested:** Unit tests check correctness vs. Flux/Zygote.

---

## âœ¨ Citation

If you use or build on this code, please cite [your name/project] or link to this repository.

---
